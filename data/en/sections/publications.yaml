# section information
section:
  name: Publications
  id: publications
  template: sections/publications.html
  enable: true
  weight: 3
  background: dark
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

pubs:
- title: "Multi-Agent Reinforcement Learning: Foundations and Modern Approaches"
  shortcut: marl_book
  url: "https://www.marl-book.com/"
  website: "https://www.marl-book.com/"
  slides: publications/slides/marl_book_slide.pdf
  bibtex: publications/bibtex/marl_book.bib
  category: Textbook
  authors: "Stefano V. Albrecht, Filippos Christianos, **Lukas Schäfer** (equal authorship)"
  year: 2024
  publisher: "Pre-print, to be published by MIT Press (scheduled for fall 2024)"

- title: Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning
  shortcut: emax
  url: "https://arxiv.org/abs/2302.03439"
  arxiv: "2302.03439"
  bibtex: publications/bibtex/emax.bib
  poster: publications/posters/emax_aamas_ala_poster.pdf
  slides: publications/slides/emax_aamas_ala_slides.pdf
  category: Workshop - Poster
  authors: "**Lukas Schäfer**, Oliver Slumbers, Stephen McAleer, Yali Du, Stefano V. Albrecht, David Mguni"
  year: 2023
  publisher: "Adaptive and Learning Agents (ALA) Workshop in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2023"
  abstract: "Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as ϵ-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exhibit lower variance compared to commonly applied target networks and we show that they lead to more stable gradients during the optimisation. We instantiate three value-based MARL algorithms with EMAX, independent DQN, VDN and QMIX, and evaluate them in 21 tasks across four environments. Using ensembles of five value functions, EMAX improves sample efficiency and final evaluation returns of these algorithms by 53%, 36%, and 498%, respectively, averaged all 21 tasks."

- title: Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments
  shortcut: tecnalia
  url: "https://arxiv.org/abs/2304.09825"
  arxiv: "2304.09825"
  bibtex: publications/bibtex/tecnalia.bib
  category: Workshop - Poster
  authors: "Alain Andres, **Lukas Schäfer**, Esther Villar-Rodriguez, Stefano V.Albrecht, Javier Del Ser"
  year: 2023
  publisher: "Adaptive and Learning Agents (ALA) Workshop in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2023"
  abstract: "One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently during online RL training both consistently improve the sample-efficiency while converging to optimal policies. Furthermore, we show that pre-training a policy from as few as two trajectories can make the difference between learning an optimal policy at the end of online training and not learning at all. Our findings motivate the widespread adoption of IL for pre-training and concurrent IL in procedurally generated environments whenever offline trajectories are available or can be generated."

- title: Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning
  shortcut: mate
  url: "https://arxiv.org/abs/2207.02249"
  arxiv: "2207.02249"
  code: https://github.com/uoe-agents/mate
  bibtex: publications/bibtex/mate.bib
  category: Preprint
  authors: "**Lukas Schäfer**, Filippos Christianos, Amos Storkey, Stefano V. Albrecht"
  year: 2022
  publisher: "arXiv"
  abstract: "Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks."

- title: Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning
  shortcut: data_collection
  url: "https://nips.cc/Conferences/2022/Schedule?showEvent=53733"
  url: "https://arxiv.org/abs/2111.14552"
  arxiv: "2111.14552"
  code: https://github.com/uoe-agents/robust_onpolicy_data_collection
  bibtex: publications/bibtex/data_collection.bib
  category: Conference - Poster
  authors: "Rujie Zhong, Duohan Zhang, **Lukas Schäfer**, Stefano V. Albrecht, Josiah P. Hanna"
  year: 2022
  publisher: "Conference on Neural Information Processing Systems (NeurIPS), 2022"
  abstract: "Reinforcement learning (RL) algorithms are often categorized as either on-policy or off-policy depending on whether they use data from a target policy of interest or from a different behavior policy. In this paper, we study a subtle distinction between on-policy data and on-policy sampling in the context of the RL sub-problem of policy evaluation. We observe that on-policy sampling may fail to match the expected distribution of on-policy data after observing only a finite number of trajectories and this failure hinders data-efficient policy evaluation. Towards improved data-efficiency, we show how non-i.i.d., off-policy sampling can produce data that more closely matches the expected on-policy data distribution and consequently increases the accuracy of the Monte Carlo estimator for policy evaluation. We introduce a method called Robust On-Policy Sampling and demonstrate theoretically and empirically that it produces data that converges faster to the expected on-policy distribution compared to on-policy sampling. Empirically, we show that this faster convergence leads to lower mean squared error policy value estimates."


- title: Task Generalisation in Multi-Agent Reinforcement Learning
  shortcut: aamas_dc
  url: "https://dl.acm.org/doi/abs/10.5555/3535850.3536132"
  bibtex: publications/bibtex/aamas_dc.bib
  slides: publications/slides/task_generalisation.pdf
  category: Doctoral Consortium
  authors: "**Lukas Schäfer**"
  year: 2022
  publisher: "Doctoral Consortium at the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2022"
  abstract: "Multi-agent reinforcement learning agents are typically trained in a single environment. As a consequence, they overfit to the training environment which results in sensitivity to perturbations and inability to generalise to similar environments. For multi-agent reinforcement learning approaches to be applicable in real-world scenarios, generalisation and robustness need to be addressed. However, unlike in supervised learning, generalisation lacks a clear definition in multi-agent reinforcement learning. We discuss the problem of task generalisation and demonstrate the difficulty of zero-shot generalisation and finetuning at the example of multi-robot warehouse coordination with preliminary results. Lastly, we discuss promising directions of research working towards generalisation of multi-agent reinforcement learning."

- title: Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration
  shortcut: derl_aamas
  url: "https://arxiv.org/abs/2107.08966"
  arxiv: "2107.08966"
  code: https://github.com/uoe-agents/derl
  bibtex: publications/bibtex/derl_aamas.bib
  slides: publications/slides/derl_aamas_slides.pdf
  category: Conference - Oral
  authors: "**Lukas Schäfer**, Filippos Christianos, Josiah P. Hanna, Stefano V. Albrecht"
  year: 2022
  publisher: "International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2022"
  abstract: "Intrinsic rewards can improve exploration in reinforcement learning, but the exploration process may suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we introduce Decoupled RL (DeRL) as a general framework which trains separate policies for intrinsically-motivated exploration and exploitation. Such decoupling allows DeRL to leverage the benefits of intrinsic rewards for exploration while demonstrating improved robustness and sample efficiency. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. Our results show that DeRL is more robust to varying scale and rate of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically-motivated baselines in fewer interactions. Lastly, we discuss the challenge of distribution shift and show that divergence constraint regularisers can successfully minimise instability caused by divergence of exploration and exploitation policies."

- title: Deep reinforcement learning for multi-agent interaction
  shortcut: agents_group
  url: "https://arxiv.org/abs/2208.01769"
  arxiv: "2208.01769"
  bibtex: publications/bibtex/agents_group.bib
  category: Journal
  authors: "Ibrahim H. Ahmed, Cillian Brewitt, Ignacio Carlucho, Filippos Christianos, Mhairi Dunion, Elliot Fosong, Samuel Garcin, Shangmin Guo, Balint Gyevnar, Trevor McInroe, Georgios Papoudakis, Arrasy Rahman, **Lukas Schäfer**, Massimiliano Tamborski, Giuseppe Vecchio, Cheng Wang, Stefano V. Albrecht"
  year: 2022
  publisher: "AI Communications Special Issue on Multi-Agent Systems Research in the UK"
  abstract: "The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions."

- title: Learning Representations for Control with Hierarchical Forward Models
  shortcut: hksl
  url: "https://arxiv.org/abs/2206.11396"
  arxiv: "2206.11396"
  code: https://github.com/trevormcinroe/hksl
  bibtex: publications/bibtex/hksl.bib
  category: Preprint
  authors: "Trevor McInroe, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2022
  publisher: "arXiv"
  abstract: "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions. Instead, we propose Hierarchical k-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks and find that HKSL either reaches higher episodic returns or converges to maximum performance more quickly than several current baselines. Also, we find that levels in HKSL's hierarchy can learn to specialize in long- or short-term consequences of agent actions, thereby providing the downstream control policy with more informative representations. Finally, we determine that communication channels between hierarchy levels organize information based on both sides of the communication process, which improves sample efficiency."

- title: Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers
  shortcut: dematic
  url: "https://arxiv.org/abs/2212.11498"
  arxiv: "2212.11498"
  bibtex: publications/bibtex/dematic.bib
  category: Preprint
  authors: "Aleksandar Krnjaic, Jonathan D. Thomas, Georgios Papoudakis, **Lukas Schäfer**, Peter Börsting, Stefano V. Albrecht"
  year: 2022
  publisher: "arXiv"
  abstract: "This project leverages advances in multi-agent reinforcement learning (MARL) to improve the efficiency and flexibility of order-picking systems for commercial warehouses. We envision a warehouse of the future in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput) under given resource constraints. Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, the MARL framework can be flexibly applied to any warehouse configuration (e.g. size, layout, number/types of workers, item replenishment frequency) and the agents learn via a process of trial-and-error how to optimally cooperate with one another. This paper details the current status of the R&D effort initiated by Dematic and the University of Edinburgh towards a general-purpose and scalable MARL solution for the order-picking problem in realistic warehouses."

- title: Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks
  shortcut: marl_benchmark
  url: "https://arxiv.org/abs/2006.07869"
  arxiv: "2006.07869"
  openreview: "https://openreview.net/forum?id=cIrPX-Sn5n"
  code: https://github.com/uoe-agents/epymarl
  bibtex: publications/bibtex/marl_benchmark.bib
  category: Conference - Poster
  authors: "Georgios Papoudakis, Filippos Christianos, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "Conference on Neural Information Processing Systems (NeurIPS), 2021 - Datasets and Benchmarks track"
  abstract: "Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we consistently evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards."

- title: Decoupling Exploration and Exploitation in Reinforcement Learning
  shortcut: derl_url
  url: "https://arxiv.org/abs/2107.08966"
  arxiv: "2107.08966"
  openreview: https://openreview.net/forum?id=NwuIIOcznYt
  code: https://github.com/uoe-agents/derl
  poster: publications/posters/derl_url.pdf
  bibtex: publications/bibtex/derl_url.bib
  category: Workshop - Poster
  authors: "**Lukas Schäfer**, Filippos Christianos, Josiah P. Hanna, Stefano V. Albrecht"
  year: 2021
  publisher: "Unsupervised Reinforcement Learning (URL) Workshop in the International Conference on Machine Learning, 2021"
  abstract: "Intrinsic rewards are commonly applied to improve exploration in reinforcement learning. However, these approaches suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains separate policies for exploration and exploitation. DeRL can be applied with on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. We show that DeRL is more robust to scaling and speed of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically motivated baselines in fewer interactions."

- title: Robust On-Policy Data Collection for Data-Efficient Policy Evaluation
  shortcut: data_collection_workshop
  url: "https://arxiv.org/abs/2111.14552"
  arxiv: "2111.14552"
  code: https://github.com/uoe-agents/robust_onpolicy_data_collection
  bibtex: publications/bibtex/data_collection_workshop.bib
  category: Workshop - Poster
  authors: "Rujie Zhong, Josiah P. Hanna, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "Workshop on Offline Reinforcement Learning in the Conference on Neural Information Processing Systems, 2021"
  abstract: "This paper considers how to complement offline reinforcement learning (RL) data with additional data collection for the task of policy evaluation. In policy evaluation, the task is to estimate the expected return of an evaluation policy on an environment of interest. Prior work on offline policy evaluation typically only considers a static dataset. We consider a setting where we can collect a small amount of additional data to combine with a potentially larger offline RL dataset. We show that simply running the evaluation policy -- on-policy data collection -- is sub-optimal for this setting. We then introduce two new data collection strategies for policy evaluation, both of which consider previously collected data when collecting future data so as to reduce distribution shift (or sampling error) in the entire dataset collected. Our empirical results show that compared to on-policy sampling, our strategies produce data with lower sampling error and generally lead to lower mean-squared error in policy evaluation for any total dataset size. We also show that these strategies can start from initial off-policy data, collect additional data, and then use both the initial and new data to produce low mean-squared error policy evaluation without using off-policy corrections."

- title: Comparative evaluation of cooperative multi-agent deep reinforcement learning algorithms
  shortcut: marl_benchmark_workshop
  url: "https://arxiv.org/abs/2006.07869"
  arxiv: "2006.07869"
  bibtex: publications/bibtex/marl_benchmark_workshop.bib
  category: Workshop - Poster
  authors: "Georgios Papoudakis, Filippos Christianos, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "Workshop on Adaptive and Learning Agents in the International Conference on Autonomous Agents and Multiagent Systems, 2021"
  abstract: "Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, and value decomposition) in a diverse range of fully-cooperative multi-agent learning tasks. Our experiments can serve as a reference for the expected performance of algorithms across different learning tasks. We also provide further insight about (1) when independent learning might be surprisingly effective despite non-stationarity, (2) when centralised training should (and shouldn’t) be applied and (3) which benefits value decomposition can bring."

- title: Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning
  shortcut: ksl
  url: "https://arxiv.org/abs/2110.04935"
  arxiv: "2110.04935"
  code: https://github.com/uoe-agents/ksl
  bibtex: publications/bibtex/ksl.bib
  category: Preprint
  authors: "Trevor McInroe, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "arXiv"
  abstract: "Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have interconnected learning burdens. Agents must learn an action-selection policy that completes their given task, which requires them to learn a representation of the state space that discerns between useful and useless information. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck that can manifest in poor sample efficiency. We present k-Step Latent (KSL), a new representation learning method that enforces temporal consistency of representations via a self-supervised auxiliary task wherein agents learn to recurrently predict action-conditioned representations of the state space. The state encoder learned by KSL produces low-dimensional representations that make optimization of the RL task more sample efficient. Altogether, KSL produces state-of-the-art results in both data efficiency and asymptotic performance in the popular PlaNet benchmark suite. Our analyses show that KSL produces encoders that generalize better to new tasks unseen during training, and its representations are more strongly tied to reward, are more invariant to perturbations in the state space, and move more smoothly through the temporal axis of the RL problem than other methods such as DrQ, RAD, CURL, and SAC-AE."

- title: Shared Experience Actor-Critic for Multi-Agent Reinforcement learning
  shortcut: seac
  url: "https://arxiv.org/abs/2006.07169"
  arxiv: "2006.07169"
  code: https://github.com/uoe-agents/seac
  poster: publications/posters/seac.pdf
  slides: publications/slides/seac.pdf
  bibtex: publications/bibtex/seac.bib
  category: Conference - Poster
  authors: "Filippos Christianos, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2020
  publisher: "Conference on Neural Information Processing Systems (NeurIPS), 2020"
  abstract: "Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all."
