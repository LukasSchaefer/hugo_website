# section information
section:
  name: Publications
  id: publications
  enable: true
  weight: 3
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

# filter buttons
buttons:
- name: All
  filter: "all"
- name: "Reinforcement Learning"
  filter: "rl"
- name: "Multi-Agent RL"
  filter: "marl"
- name: "Imitation Learning"
  filter: "il"

# your publications
publications:
- title: "Multi-Agent Reinforcement Learning: Foundations and Modern Approaches"
  publishedIn:
    name: MIT Press
    date: 2024
    url: https://www.marl-book.com/
  authors:
  - name: Stefano V. Albrecht
    # url:
  - name: Filippos Christianos
    # url:
  - name: Lukas Schäfer
    # url:
  paper:
    abstract: "This book provides a comprehensive introduction to multi-agent reinforcement learning (MARL), a rapidly growing field that combines insights from game theory, reinforcement learning, and multi-agent systems. The book covers the foundations of MARL, including the key concepts, algorithms, and challenges, and presents a detailed overview of contemporary approaches of deep MARL research."
    url: https://www.marl-book.com/
  categories: ["marl","rl"]
  tags: ["Multi-Agent Reinforcement Learning"]

- title: Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments
  publishedIn:
    name: "Neurocomputing Journal"
    date: 2024
    url: "https://arxiv.org/abs/2304.09825"
  authors:
    - name: "Alain Andres"
    - name: "Lukas Schäfer"
    - name: Esther Villar-Rodriguez
    - name: Stefano V. Albrecht
    - name: Javier Del Ser
  paper:
    abstract: "One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently during online RL training both consistently improve the sample-efficiency while converging to optimal policies. Furthermore, we show that pre-training a policy from as few as two trajectories can make the difference between learning an optimal policy at the end of online training and not learning at all. Our findings motivate the widespread adoption of IL for pre-training and concurrent IL in procedurally generated environments whenever offline trajectories are available or can be generated."
    url: "https://arxiv.org/abs/2304.09825"
  categories: ["rl", "il"]
  tags: ["Reinforcement Learning", "Imitation Learning"]

- title: Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games
  publishedIn:
    name: arXiv
    date: 2023
    url: "https://arxiv.org/abs/2312.02312"
  authors:
    - name: "Lukas Schäfer"
    - name: Logan Jones
    - name: Anssi Kanervisto
    - name: Yuhan Cao
    - name: Tabish Rashid
    - name: Raluca Georgescu
    - name: Dave Bignell
    - name: Siddhartha Sen
    - name: Andrea Treviño Gavito
    - name: Sam Devlin
  paper:
    abstract: "Video games have served as useful benchmarks for the decision making community, but going beyond Atari games towards training agents in modern games has been prohibitively expensive for the vast majority of the research community. Recent progress in the research, development and open release of large vision models has the potential to amortize some of these costs across the community. However, it is currently unclear which of these models have learnt representations that retain information critical for sequential decision making. Towards enabling wider participation in the research of gameplaying agents in modern games, we present a systematic study of imitation learning with publicly available visual encoders compared to the typical, task-specific, end-to-end training approach in Minecraft, Minecraft Dungeons and Counter-Strike: Global Offensive."
    url: "https://arxiv.org/abs/2312.02312"
  categories: ["il"]
  tags: ["Imitation Learning", "Visual Encoders"]

- title: Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning
  publishedIn:
    name: "Workshop on Generalization in Planning in the Conference on Neural Information Processing Systems (NeurIPS)"
    date: 2023
    url: "https://arxiv.org/abs/2207.02249"
  authors:
    - name: "Lukas Schäfer"
    - name: Filippos Christianos
    - name: Amos Storkey
    - name: Stefano V. Albrecht
  code: https://github.com/uoe-agents/mate
  paper:
    abstract: "Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks."
    url: "https://arxiv.org/abs/2207.02249"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Generalisation"]

- title: Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning
  publishedIn:
    name: "Adaptive and Learning Agents (ALA) Workshop in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"
    date: 2023
    url: "https://arxiv.org/abs/2302.03439"
  authors:
    - name: "Lukas Schäfer"
    - name: Oliver Slumbers
    - name: Stephen McAleer
    - name: Yali Du
    - name: Stefano V. Albrecht
    - name: David Mguni
  paper:
    abstract: "Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as ϵ-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exhibit lower variance compared to commonly applied target networks and we show that they lead to more stable gradients during the optimisation. We instantiate three value-based MARL algorithms with EMAX, independent DQN, VDN and QMIX, and evaluate them in 21 tasks across four environments. Using ensembles of five value functions, EMAX improves sample efficiency and final evaluation returns of these algorithms by 53%, 36%, and 498%, respectively, averaged all 21 tasks."
    url: "https://arxiv.org/abs/2302.03439"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Exploration"]

- title: Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning
  publishedIn:
    name: "Transactions on Machine Learning Research (TMLR) Journal"
    date: 2023
    url: "https://arxiv.org/abs/2206.11396"
  authors:
    - name: "Trevor McInroe"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  code: https://github.com/trevormcinroe/hksl
  paper:
    abstract: "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions. Instead, we propose Hierarchical k-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks and find that HKSL either reaches higher episodic returns or converges to maximum performance more quickly than several current baselines. Also, we find that levels in HKSL's hierarchy can learn to specialize in long- or short-term consequences of agent actions, thereby providing the downstream control policy with more informative representations. Finally, we determine that communication channels between hierarchy levels organize information based on both sides of the communication process, which improves sample efficiency."
    url: "https://arxiv.org/abs/2206.11396"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Representation Learning"]

- title: Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning
  publishedIn:
    name: "Conference on Neural Information Processing Systems (NeurIPS)"
    date: 2022
    url: "https://arxiv.org/abs/2111.14552"
  authors:
    - name: "Rujie Zhong"
    - name: "Duohan Zhang"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
    - name: "Josiah P. Hanna"
  code: https://github.com/uoe-agents/robust_onpolicy_data_collection
  paper:
    abstract: "Reinforcement learning (RL) algorithms are often categorized as either on-policy or off-policy depending on whether they use data from a target policy of interest or from a different behavior policy. In this paper, we study a subtle distinction between on-policy data and on-policy sampling in the context of the RL sub-problem of policy evaluation. We observe that on-policy sampling may fail to match the expected distribution of on-policy data after observing only a finite number of trajectories and this failure hinders data-efficient policy evaluation. Towards improved data-efficiency, we show how non-i.i.d., off-policy sampling can produce data that more closely matches the expected on-policy data distribution and consequently increases the accuracy of the Monte Carlo estimator for policy evaluation. We introduce a method called Robust On-Policy Sampling and demonstrate theoretically and empirically that it produces data that converges faster to the expected on-policy distribution compared to on-policy sampling. Empirically, we show that this faster convergence leads to lower mean squared error policy value estimates."
    url: "https://arxiv.org/abs/2111.14552"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Policy Evaluation"]

- title: Task Generalisation in Multi-Agent Reinforcement Learning
  publishedIn:
    name: "Doctoral Consortium at the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"
    date: 2022
    url: "https://dl.acm.org/doi/abs/10.5555/3535850.3536132"
  authors:
    - name: "Lukas Schäfer"
  paper:
    abstract: "Multi-agent reinforcement learning agents are typically trained in a single environment. As a consequence, they overfit to the training environment which results in sensitivity to perturbations and inability to generalise to similar environments. For multi-agent reinforcement learning approaches to be applicable in real-world scenarios, generalisation and robustness need to be addressed. However, unlike in supervised learning, generalisation lacks a clear definition in multi-agent reinforcement learning. We discuss the problem of task generalisation and demonstrate the difficulty of zero-shot generalisation and finetuning at the example of multi-robot warehouse coordination with preliminary results. Lastly, we discuss promising directions of research working towards generalisation of multi-agent reinforcement learning."
    url: "https://dl.acm.org/doi/abs/10.5555/3535850.3536132"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Generalisation"]

- title: Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration
  publishedIn:
    name: "International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"
    date: 2022
    url: "https://arxiv.org/abs/2107.08966"
  authors:
    - name: "Lukas Schäfer"
    - name: "Filippos Christianos"
    - name: "Josiah P. Hanna"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/derl
  paper:
    abstract: "Intrinsic rewards can improve exploration in reinforcement learning, but the exploration process may suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we introduce Decoupled RL (DeRL) as a general framework which trains separate policies for intrinsically-motivated exploration and exploitation. Such decoupling allows DeRL to leverage the benefits of intrinsic rewards for exploration while demonstrating improved robustness and sample efficiency. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. Our results show that DeRL is more robust to varying scale and rate of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically-motivated baselines in fewer interactions. Lastly, we discuss the challenge of distribution shift and show that divergence constraint regularisers can successfully minimise instability caused by divergence of exploration and exploitation policies."
    url: "https://arxiv.org/abs/2107.08966"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Exploration"]

- title: Deep reinforcement learning for multi-agent interaction
  publishedIn:
    name: "AI Communications Special Issue on Multi-Agent Systems Research in the UK"
    date: 2022
    url: "https://arxiv.org/abs/2208.01769"
  authors:
    - name: "Ibrahim H. Ahmed"
    - name: "Cillian Brewitt"
    - name: "Ignacio Carlucho"
    - name: "Filippos Christianos"
    - name: "Mhairi Dunion"
    - name: "Elliot Fosong"
    - name: "Samuel Garcin"
    - name: "Shangmin Guo"
    - name: "Balint Gyevnar"
    - name: "Trevor McInroe"
    - name: "Georgios Papoudakis"
    - name: "Arrasy Rahman"
    - name: "Lukas Schäfer"
    - name: "Massimiliano Tamborski"
    - name: "Giuseppe Vecchio"
    - name: "Cheng Wang"
    - name: "Stefano V. Albrecht"
  paper:
    abstract: "The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions."
    url: "https://arxiv.org/abs/2208.01769"
  categories: ["marl", "rl"]
  tags: ["Multi-Agent Reinforcement Learning", "Reinforcement Learning"]

- title: Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers
  publishedIn:
    name: "International Conference on Intelligent Robots and Systems"
    date: 2024
    url: "https://arxiv.org/abs/2212.11498"
  authors:
    - name: "Aleksandar Krnjaic"
    - name: "Raul D. Steleac"
    - name: "Jonathan D. Thomas"
    - name: "Georgios Papoudakis"
    - name: "Lukas Schäfer"
    - name: "Andrew Wing Keung To"
    - name: "Kuan-Ho Lao"
    - name: "Murat Cubuktepe"
    - name: "Matthew Haley"
    - name: "Peter Börsting"
    - name: "Stefano V. Albrecht"
  paper:
    abstract: "We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorithms achieve significant gains in sample efficiency and overall pick rates over baseline MARL algorithms in diverse warehouse configurations, and substantially outperform two established industry heuristics for order-picking systems."
    url: "https://arxiv.org/abs/2212.11498"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Warehouse Logistics"]

- title: Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks
  publishedIn:
    name: "Conference on Neural Information Processing Systems (NeurIPS)"
    date: 2021
    url: "https://arxiv.org/abs/2006.07869"
  authors:
    - name: "Georgios Papoudakis"
    - name: "Filippos Christianos"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/epymarl
  paper:
    abstract: "Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we consistently evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards."
    url: "https://arxiv.org/abs/2006.07869"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Benchmark"]

- title: Decoupling Exploration and Exploitation in Reinforcement Learning
  publishedIn:
    name: "Unsupervised Reinforcement Learning (URL) Workshop in the International Conference on Machine Learning"
    date: 2021
    url: "https://arxiv.org/abs/2107.08966"
  authors:
    - name: "Lukas Schäfer"
    - name: "Filippos Christianos"
    - name: "Josiah P. Hanna"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/derl
  paper:
    abstract: "Intrinsic rewards are commonly applied to improve exploration in reinforcement learning. However, these approaches suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains separate policies for exploration and exploitation. DeRL can be applied with on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. We show that DeRL is more robust to scaling and speed of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically motivated baselines in fewer interactions."
    url: "https://arxiv.org/abs/2107.08966"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Exploration"]

- title: Robust On-Policy Data Collection for Data-Efficient Policy Evaluation
  publishedIn:
    name: "Workshop on Offline Reinforcement Learning in the Conference on Neural Information Processing Systems"
    date: 2021
    url: "https://arxiv.org/abs/2111.14552"
  authors:
    - name: "Rujie Zhong"
    - name: "Josiah P. Hanna"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/robust_onpolicy_data_collection
  paper:
    abstract: "This paper considers how to complement offline reinforcement learning (RL) data with additional data collection for the task of policy evaluation. In policy evaluation, the task is to estimate the expected return of an evaluation policy on an environment of interest. Prior work on offline policy evaluation typically only considers a static dataset. We consider a setting where we can collect a small amount of additional data to combine with a potentially larger offline RL dataset. We show that simply running the evaluation policy -- on-policy data collection -- is sub-optimal for this setting. We then introduce two new data collection strategies for policy evaluation, both of which consider previously collected data when collecting future data so as to reduce distribution shift (or sampling error) in the entire dataset collected. Our empirical results show that compared to on-policy sampling, our strategies produce data with lower sampling error and generally lead to lower mean-squared error in policy evaluation for any total dataset size. We also show that these strategies can start from initial off-policy data, collect additional data, and then use both the initial and new data to produce low mean-squared error policy evaluation without using off-policy corrections."
    url: "https://arxiv.org/abs/2111.14552"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Policy Evaluation"]

- title: Comparative evaluation of cooperative multi-agent deep reinforcement learning algorithms
  publishedIn:
    name: "Workshop on Adaptive and Learning Agents in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"
    date: 2021
    url: "https://arxiv.org/abs/2006.07869"
  authors:
    - name: "Georgios Papoudakis"
    - name: "Filippos Christianos"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  paper:
    abstract: "Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, and value decomposition) in a diverse range of fully-cooperative multi-agent learning tasks. Our experiments can serve as a reference for the expected performance of algorithms across different learning tasks. We also provide further insight about (1) when independent learning might be surprisingly effective despite non-stationarity, (2) when centralised training should (and shouldn’t) be applied and (3) which benefits value decomposition can bring."
    url: "https://arxiv.org/abs/2006.07869"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Benchmark"]

- title: Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning
  publishedIn:
    name: "arXiv"
    date: 2021
    url: "https://arxiv.org/abs/2110.04935"
  authors:
    - name: "Trevor McInroe"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/ksl
  paper:
    abstract: "Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have interconnected learning burdens. Agents must learn an action-selection policy that completes their given task, which requires them to learn a representation of the state space that discerns between useful and useless information. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck that can manifest in poor sample efficiency. We present k-Step Latent (KSL), a new representation learning method that enforces temporal consistency of representations via a self-supervised auxiliary task wherein agents learn to recurrently predict action-conditioned representations of the state space. The state encoder learned by KSL produces low-dimensional representations that make optimization of the RL task more sample efficient. Altogether, KSL produces state-of-the-art results in both data efficiency and asymptotic performance in the popular PlaNet benchmark suite. Our analyses show that KSL produces encoders that generalize better to new tasks unseen during training, and its representations are more strongly tied to reward, are more invariant to perturbations in the state space, and move more smoothly through the temporal axis of the RL problem than other methods such as DrQ, RAD, CURL, and SAC-AE."
    url: "https://arxiv.org/abs/2110.04935"
  categories: ["rl"]
  tags: ["Reinforcement Learning", "Representation Learning"]

- title: Shared Experience Actor-Critic for Multi-Agent Reinforcement learning
  publishedIn:
    name: "Conference on Neural Information Processing Systems (NeurIPS)"
    date: 2020
    url: "https://arxiv.org/abs/2006.07169"
  authors:
    - name: "Filippos Christianos"
    - name: "Lukas Schäfer"
    - name: "Stefano V. Albrecht"
  code: https://github.com/uoe-agents/seac
  paper:
    abstract: "Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all."
    url: "https://arxiv.org/abs/2006.07169"
  categories: ["marl"]
  tags: ["Multi-Agent Reinforcement Learning", "Exploration"]
