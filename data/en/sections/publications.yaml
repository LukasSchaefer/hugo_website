# section information
section:
  name: Publications
  id: publications
  template: sections/publications.html
  enable: true
  weight: 3
  background: dark
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

pubs:
- title: Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning
  shortcut: mate
  url: "https://arxiv.org/abs/2207.02249"
  arxiv: "2207.02249"
  code: https://github.com/uoe-agents/mate
  bibtex: publications/bibtex/mate.bib
  category: Preprint
  authors: "**Lukas Schäfer**, Filippos Christianos, Amos Storkey, Stefano V. Albrecht"
  year: 2022
  publisher: "arXiv"
  abstract: "Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks."

- title: Task Generalisation in Multi-Agent Reinforcement Learning
  shortcut: aamas_dc
  url: "https://dl.acm.org/doi/abs/10.5555/3535850.3536132"
  bibtex: publications/bibtex/aamas_dc.bib
  category: Doctoral Consortium
  authors: "**Lukas Schäfer**"
  year: 2022
  publisher: "Doctoral Consortium at the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2022"
  abstract: "Multi-agent reinforcement learning agents are typically trained in a single environment. As a consequence, they overfit to the training environment which results in sensitivity to perturbations and inability to generalise to similar environments. For multi-agent reinforcement learning approaches to be applicable in real-world scenarios, generalisation and robustness need to be addressed. However, unlike in supervised learning, generalisation lacks a clear definition in multi-agent reinforcement learning. We discuss the problem of task generalisation and demonstrate the difficulty of zero-shot generalisation and finetuning at the example of multi-robot warehouse coordination with preliminary results. Lastly, we discuss promising directions of research working towards generalisation of multi-agent reinforcement learning."

- title: Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration
  shortcut: derl_aamas
  url: "https://arxiv.org/abs/2107.08966"
  arxiv: "2107.08966"
  code: https://github.com/uoe-agents/derl
  bibtex: publications/bibtex/derl_aamas.bib
  category: Conference - Oral
  authors: "**Lukas Schäfer**, Filippos Christianos, Josiah P. Hanna, Stefano V. Albrecht"
  year: 2022
  publisher: "International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2022"
  abstract: "Intrinsic rewards can improve exploration in reinforcement learning, but the exploration process may suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we introduce Decoupled RL (DeRL) as a general framework which trains separate policies for intrinsically-motivated exploration and exploitation. Such decoupling allows DeRL to leverage the benefits of intrinsic rewards for exploration while demonstrating improved robustness and sample efficiency. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. Our results show that DeRL is more robust to varying scale and rate of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically-motivated baselines in fewer interactions. Lastly, we discuss the challenge of distribution shift and show that divergence constraint regularisers can successfully minimise instability caused by divergence of exploration and exploitation policies."

- title: Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks
  shortcut: marl_benchmark
  url: "https://arxiv.org/abs/2006.07869"
  arxiv: "2006.07869"
  openreview: "https://openreview.net/forum?id=cIrPX-Sn5n"
  code: https://github.com/uoe-agents/epymarl
  bibtex: publications/bibtex/marl_benchmark.bib
  category: Conference - Poster
  authors: "Georgios Papoudakis, Filippos Christianos, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "Conference on Neural Information Processing Systems (NeurIPS), 2021 - Datasets and Benchmarks track"
  abstract: "Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we consistently evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards."

- title: Decoupling Exploration and Exploitation in Reinforcement Learning
  shortcut: derl_url
  url: "https://arxiv.org/abs/2107.08966"
  arxiv: "2107.08966"
  openreview: https://openreview.net/forum?id=NwuIIOcznYt
  code: https://github.com/uoe-agents/derl
  poster: publications/posters/derl_url.pdf
  bibtex: publications/bibtex/derl_url.bib
  category: Workshop - Poster
  authors: "**Lukas Schäfer**, Filippos Christianos, Josiah P. Hanna, Stefano V. Albrecht"
  year: 2021
  publisher: "Unsupervised Reinforcement Learning (URL) Workshop in the International Conference on Machine Learning, 2021"
  abstract: "Intrinsic rewards are commonly applied to improve exploration in reinforcement learning. However, these approaches suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains separate policies for exploration and exploitation. DeRL can be applied with on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. We show that DeRL is more robust to scaling and speed of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically motivated baselines in fewer interactions."

- title: Robust On-Policy Data Collection for Data-Efficient Policy Evaluation
  shortcut: data_collection
  url: "https://arxiv.org/abs/2111.14552"
  arxiv: "2111.14552"
  code: https://github.com/uoe-agents/robust_onpolicy_data_collection
  bibtex: publications/bibtex/data_collection.bib
  category: Workshop - Poster
  authors: "Rujie Zhong, Josiah P. Hanna, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "Workshop on Offline Reinforcement Learning in the Conference on Neural Information Processing Systems, 2021"
  abstract: "This paper considers how to complement offline reinforcement learning (RL) data with additional data collection for the task of policy evaluation. In policy evaluation, the task is to estimate the expected return of an evaluation policy on an environment of interest. Prior work on offline policy evaluation typically only considers a static dataset. We consider a setting where we can collect a small amount of additional data to combine with a potentially larger offline RL dataset. We show that simply running the evaluation policy -- on-policy data collection -- is sub-optimal for this setting. We then introduce two new data collection strategies for policy evaluation, both of which consider previously collected data when collecting future data so as to reduce distribution shift (or sampling error) in the entire dataset collected. Our empirical results show that compared to on-policy sampling, our strategies produce data with lower sampling error and generally lead to lower mean-squared error in policy evaluation for any total dataset size. We also show that these strategies can start from initial off-policy data, collect additional data, and then use both the initial and new data to produce low mean-squared error policy evaluation without using off-policy corrections."

- title: Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning
  shortcut: ksl
  url: "https://arxiv.org/abs/2110.04935"
  arxiv: "2110.04935"
  code: https://github.com/uoe-agents/ksl
  bibtex: publications/bibtex/ksl.bib
  category: Preprint
  authors: "Trevor McInroe, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2021
  publisher: "arXiv"
  abstract: "Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have interconnected learning burdens. Agents must learn an action-selection policy that completes their given task, which requires them to learn a representation of the state space that discerns between useful and useless information. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck that can manifest in poor sample efficiency. We present k-Step Latent (KSL), a new representation learning method that enforces temporal consistency of representations via a self-supervised auxiliary task wherein agents learn to recurrently predict action-conditioned representations of the state space. The state encoder learned by KSL produces low-dimensional representations that make optimization of the RL task more sample efficient. Altogether, KSL produces state-of-the-art results in both data efficiency and asymptotic performance in the popular PlaNet benchmark suite. Our analyses show that KSL produces encoders that generalize better to new tasks unseen during training, and its representations are more strongly tied to reward, are more invariant to perturbations in the state space, and move more smoothly through the temporal axis of the RL problem than other methods such as DrQ, RAD, CURL, and SAC-AE."

- title: Shared Experience Actor-Critic for Multi-Agent Reinforcement learning
  shortcut: seac
  url: "https://arxiv.org/abs/2006.07169"
  arxiv: "2006.07169"
  code: https://github.com/uoe-agents/seac
  poster: publications/posters/seac.pdf
  slides: publications/slides/seac.pdf
  bibtex: publications/bibtex/seac.bib
  category: Conference - Poster
  authors: "Filippos Christianos, **Lukas Schäfer**, Stefano V. Albrecht"
  year: 2020
  publisher: "Conference on Neural Information Processing Systems (NeurIPS), 2020"
  abstract: "Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all."
